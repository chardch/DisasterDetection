{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model weights\n",
    "model = load_model('tomnod_everything_relu_Adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "100/100 [==============================] - 14s 140ms/step\n",
      "[0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]\n",
      "[]\n",
      "[0.11823715090751648, 0.9619999933242798]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model as-is on original dataset\n",
    "test_dir = '/Users/apando/school/cs230/project/repos/DamageDetection/test'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    test_dir, \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary', shuffle=True)\n",
    "test_results = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "val_targ = []\n",
    "for i in range(0,100):\n",
    "    val_targ.extend(np.array(test_generator[i][1]))\n",
    "\n",
    "val_predict = (np.asarray(test_results)).round().flatten()\n",
    "\n",
    "print(val_targ[100:200])\n",
    "print(val_predict[100:200])\n",
    "\n",
    "print(test_results)\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "100/100 [==============================] - 13s 130ms/step\n",
      "[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1.]\n",
      "F1: 0.4950884086444008, Recall: 0.504, Precision: 0.4864864864864865, Accuracy: 0.486\n"
     ]
    }
   ],
   "source": [
    "# F1 score on original dataset\n",
    "from sklearn import metrics\n",
    "\n",
    "test_dir = '/Users/apando/school/cs230/project/repos/DamageDetection/test'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    test_dir, \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary', shuffle=True)\n",
    "\n",
    "predictions = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "val_targ = []\n",
    "for i in range(0,100):\n",
    "    val_targ.extend(np.array(test_generator[i][1]))\n",
    "\n",
    "val_predict = (np.asarray(predictions)).round().flatten()\n",
    "\n",
    "print(val_targ[100:200])\n",
    "print(val_predict[100:200])\n",
    "\n",
    "val_f1 = metrics.f1_score(val_targ, val_predict)\n",
    "val_recall = metrics.recall_score(val_targ, val_predict)\n",
    "val_precision = metrics.precision_score(val_targ, val_predict)\n",
    "val_acc = metrics.accuracy_score(val_targ, val_predict)\n",
    "\n",
    "print(\"F1: {0}, Recall: {1}, Precision: {2}, Accuracy: {3}\".format(val_f1, val_recall, val_precision, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8444 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.190115223824978, 0.5475000023841858]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model as-is on a subset of ABCD dataset\n",
    "abcddataset_test_dir = '/Users/apando/school/cs230/project/datasets/abcddataset/ABCDdataset/resized/post-event'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    abcddataset_test_dir, \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "test_results = model.evaluate_generator(test_generator, steps=20)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8444 images belonging to 2 classes.\n",
      "423/423 [==============================] - 55s 131ms/step\n"
     ]
    }
   ],
   "source": [
    "# F1 score on ABCD dataset\n",
    "from sklearn import metrics\n",
    "\n",
    "abcddataset_test_dir = '/Users/apando/school/cs230/project/datasets/abcddataset/ABCDdataset/resized/post-event'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    abcddataset_test_dir, \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary', shuffle=False)\n",
    "l\n",
    "predictions = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "val_targ = []\n",
    "for i in range(0,50):\n",
    "    val_targ.extend(np.array(test_generator[i][1]))\n",
    "\n",
    "val_predict = (np.asarray(predictions)).round().flatten()\n",
    "\n",
    "print(val_targ[100:200])\n",
    "print(val_predict[100:200])\n",
    "\n",
    "val_f1 = metrics.f1_score(val_targ, val_predict)\n",
    "val_recall = metrics.recall_score(val_targ, val_predict)\n",
    "val_precision = metrics.precision_score(val_targ, val_predict)\n",
    "val_acc = metrics.accuracy_score(val_targ, val_predict)\n",
    "\n",
    "print(\"F1: {0}, Recall: {1}, Precision: {2}, Accuracy: {3}\".format(val_f1, val_recall, val_precision, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 220 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3643310178409924, 0.5318181839856234]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model as-is on a subset of Hurricane Maria dataset\n",
    "maria_dir = '/Users/apando/school/cs230/project/datasets/hurricane_maria/building_crops_small/test'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    maria_dir, \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "test_results = model.evaluate_generator(test_generator, steps=len(test_generator))\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 220 images belonging to 2 classes.\n",
      "11/11 [==============================] - 1s 132ms/step\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0.]\n",
      "F1: 0.45502645502645506, Recall: 0.39814814814814814, Precision: 0.5308641975308642, Accuracy: 0.5318181818181819\n"
     ]
    }
   ],
   "source": [
    "# F1 score on Maria small/clean test dataset\n",
    "from sklearn import metrics\n",
    "\n",
    "maria_test_dir = '/Users/apando/school/cs230/project/datasets/hurricane_maria/building_crops_small/test'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    maria_test_dir, \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary', shuffle=False)\n",
    "\n",
    "predictions = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "val_targ = []\n",
    "for i in range(0,len(test_generator)):\n",
    "    val_targ.extend(np.array(test_generator[i][1]))\n",
    "\n",
    "val_predict = (np.asarray(predictions)).round().flatten()\n",
    "\n",
    "print(val_targ[100:200])\n",
    "print(val_predict[100:200])\n",
    "\n",
    "val_f1 = metrics.f1_score(val_targ, val_predict)\n",
    "val_recall = metrics.recall_score(val_targ, val_predict)\n",
    "val_precision = metrics.precision_score(val_targ, val_predict)\n",
    "val_acc = metrics.accuracy_score(val_targ, val_predict)\n",
    "\n",
    "print(\"F1: {0}, Recall: {1}, Precision: {2}, Accuracy: {3}\".format(val_f1, val_recall, val_precision, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
